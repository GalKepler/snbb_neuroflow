{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Core Infrastructure Testing\n",
    "\n",
    "This notebook tests all components added in Phase 1 of the workflow orchestration system.\n",
    "\n",
    "## Components Tested:\n",
    "1. Pipeline Registry\n",
    "2. WorkflowRun Model\n",
    "3. Enhanced Models (Session, Subject, PipelineRun)\n",
    "4. StateManager Workflow Operations\n",
    "5. Celery Configuration\n",
    "\n",
    "**Prerequisites:**\n",
    "- Database initialized\n",
    "- neuroflow.yaml configured\n",
    "- Redis running (for Celery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from neuroflow.config import NeuroflowConfig\n",
    "from neuroflow.core.state import StateManager\n",
    "from neuroflow.models import (\n",
    "    WorkflowRun,\n",
    "    WorkflowRunStatus,\n",
    "    Session,\n",
    "    SessionStatus,\n",
    "    Subject,\n",
    "    SubjectStatus,\n",
    "    PipelineRun,\n",
    "    PipelineRunStatus,\n",
    ")\n",
    "from neuroflow.orchestrator.registry import (\n",
    "    get_registry,\n",
    "    PipelineRegistry,\n",
    "    PipelineLevel,\n",
    "    PipelineStage,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Pipeline Registry\n",
    "\n",
    "The Pipeline Registry provides centralized pipeline definitions with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2026-02-07 19:30:56\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mregistry.registered           \u001b[0m \u001b[36mpipeline\u001b[0m=\u001b[35mbids_conversion\u001b[0m\n",
      "\u001b[2m2026-02-07 19:30:56\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mregistry.registered           \u001b[0m \u001b[36mpipeline\u001b[0m=\u001b[35mqsiprep\u001b[0m\n",
      "\u001b[2m2026-02-07 19:30:56\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mregistry.registered           \u001b[0m \u001b[36mpipeline\u001b[0m=\u001b[35mqsirecon\u001b[0m\n",
      "\u001b[2m2026-02-07 19:30:56\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mregistry.registered           \u001b[0m \u001b[36mpipeline\u001b[0m=\u001b[35mqsiparc\u001b[0m\n",
      "\n",
      "üîç Testing Pipeline Registry\n",
      "============================================================\n",
      "\n",
      "üìã Registered Pipelines:\n",
      "  - bids_conversion: BIDS Conversion (HeudiConv)\n",
      "    Stage: BIDS_CONVERSION, Level: session\n",
      "    Queue: bids, Timeout: 60min\n",
      "\n",
      "  - qsiprep: QSIPrep Preprocessing\n",
      "    Stage: PREPROCESSING, Level: subject\n",
      "    Queue: heavy_processing, Timeout: 1440min\n",
      "    Depends on: bids_conversion\n",
      "\n",
      "  - qsirecon: QSIRecon Reconstruction\n",
      "    Stage: RECONSTRUCTION, Level: session\n",
      "    Queue: processing, Timeout: 720min\n",
      "    Depends on: qsiprep\n",
      "\n",
      "  - qsiparc: QSIParc Parcellation\n",
      "    Stage: PARCELLATION, Level: session\n",
      "    Queue: processing, Timeout: 240min\n",
      "    Depends on: qsirecon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the global registry instance\n",
    "registry = get_registry()\n",
    "\n",
    "print(\"\\nüîç Testing Pipeline Registry\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all registered pipelines\n",
    "print(\"\\nüìã Registered Pipelines:\")\n",
    "for pipeline in registry.get_all():\n",
    "    print(f\"  - {pipeline.name}: {pipeline.display_name}\")\n",
    "    print(f\"    Stage: {pipeline.stage.name}, Level: {pipeline.level.value}\")\n",
    "    print(f\"    Queue: {pipeline.queue}, Timeout: {pipeline.timeout_minutes}min\")\n",
    "    if pipeline.depends_on:\n",
    "        print(f\"    Depends on: {', '.join(pipeline.depends_on)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Testing Dependency Resolution\n",
      "============================================================\n",
      "\n",
      "Dependencies for 'qsiparc':\n",
      "  - bids_conversion (Stage 1)\n",
      "  - qsiprep (Stage 2)\n",
      "  - qsirecon (Stage 3)\n",
      "\n",
      "üìä Execution Order (Topological Sort):\n",
      "  1. bids_conversion (Stage: BIDS_CONVERSION)\n",
      "  2. qsiprep (Stage: PREPROCESSING)\n",
      "  3. qsirecon (Stage: RECONSTRUCTION)\n",
      "  4. qsiparc (Stage: PARCELLATION)\n"
     ]
    }
   ],
   "source": [
    "# Test dependency resolution\n",
    "print(\"\\nüîó Testing Dependency Resolution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline_name = \"qsiparc\"\n",
    "dependencies = registry.get_dependencies(pipeline_name)\n",
    "print(f\"\\nDependencies for '{pipeline_name}':\")\n",
    "for dep in dependencies:\n",
    "    print(f\"  - {dep.name} (Stage {dep.stage.value})\")\n",
    "\n",
    "# Test execution order (topological sort)\n",
    "print(\"\\nüìä Execution Order (Topological Sort):\")\n",
    "execution_order = registry.get_execution_order()\n",
    "for i, pipeline in enumerate(execution_order, 1):\n",
    "    print(f\"  {i}. {pipeline.name} (Stage: {pipeline.stage.name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Validating Dependencies\n",
      "============================================================\n",
      "\n",
      "‚úÖ All dependencies valid!\n"
     ]
    }
   ],
   "source": [
    "# Validate dependencies\n",
    "print(\"\\n‚úì Validating Dependencies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = registry.validate_dependencies()\n",
    "if errors:\n",
    "    print(\"\\n‚ùå Dependency Errors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All dependencies valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è Testing Retry Policies\n",
      "============================================================\n",
      "\n",
      "QSIPrep Retry Policy:\n",
      "  Max attempts: 3\n",
      "  Initial delay: 1800s\n",
      "  Max delay: 7200s\n",
      "  Exponential backoff: True\n",
      "\n",
      "  Delay schedule:\n",
      "    Attempt 1: 1800s (30.0 minutes)\n",
      "    Attempt 2: 3600s (60.0 minutes)\n",
      "    Attempt 3: 7200s (120.0 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Test retry policy\n",
    "print(\"\\n‚è±Ô∏è Testing Retry Policies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qsiprep = registry.get(\"qsiprep\")\n",
    "print(f\"\\nQSIPrep Retry Policy:\")\n",
    "print(f\"  Max attempts: {qsiprep.retry_policy.max_attempts}\")\n",
    "print(f\"  Initial delay: {qsiprep.retry_policy.initial_delay_seconds}s\")\n",
    "print(f\"  Max delay: {qsiprep.retry_policy.max_delay_seconds}s\")\n",
    "print(f\"  Exponential backoff: {qsiprep.retry_policy.exponential_backoff}\")\n",
    "\n",
    "print(\"\\n  Delay schedule:\")\n",
    "for attempt in range(1, qsiprep.retry_policy.max_attempts + 1):\n",
    "    delay = qsiprep.retry_policy.get_delay(attempt)\n",
    "    print(f\"    Attempt {attempt}: {delay}s ({delay/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíª Testing Resource Requirements\n",
      "============================================================\n",
      "\n",
      "BIDS Conversion (HeudiConv):\n",
      "  CPUs: 1-2\n",
      "  Memory: 4-8 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 30 minutes\n",
      "\n",
      "QSIPrep Preprocessing:\n",
      "  CPUs: 8-16\n",
      "  Memory: 16-32 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 720 minutes\n",
      "\n",
      "QSIRecon Reconstruction:\n",
      "  CPUs: 4-8\n",
      "  Memory: 8-16 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 480 minutes\n"
     ]
    }
   ],
   "source": [
    "# Test resource requirements\n",
    "print(\"\\nüíª Testing Resource Requirements\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pipeline_name in [\"bids_conversion\", \"qsiprep\", \"qsirecon\"]:\n",
    "    pipeline = registry.get(pipeline_name)\n",
    "    resources = pipeline.resources\n",
    "    print(f\"\\n{pipeline.display_name}:\")\n",
    "    print(f\"  CPUs: {resources.min_cpus}-{resources.max_cpus}\")\n",
    "    print(f\"  Memory: {resources.min_memory_gb}-{resources.max_memory_gb} GB\")\n",
    "    print(f\"  GPU Required: {resources.requires_gpu}\")\n",
    "    print(f\"  Est. Duration: {resources.estimated_duration_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Database Models and Migrations\n",
    "\n",
    "Testing the new WorkflowRun model and enhanced Session/Subject models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No configuration file found. Searched:\n  - ./neuroflow.yaml\n  - /home/galkepler/.config/neuroflow/neuroflow.yaml\n  - /etc/neuroflow/neuroflow.yaml",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize config and state manager\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m config = \u001b[43mNeuroflowConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m state = StateManager(config)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müóÑÔ∏è Testing Database Models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/neuroflow/neuroflow/config.py:184\u001b[39m, in \u001b[36mNeuroflowConfig.find_and_load\u001b[39m\u001b[34m(cls, config_path)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mand\u001b[39;00m Path(p).exists():\n\u001b[32m    182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_yaml(p)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    185\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNo configuration file found. Searched:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    186\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m search_paths \u001b[38;5;28;01mif\u001b[39;00m p)\n\u001b[32m    187\u001b[39m )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No configuration file found. Searched:\n  - ./neuroflow.yaml\n  - /home/galkepler/.config/neuroflow/neuroflow.yaml\n  - /etc/neuroflow/neuroflow.yaml"
     ]
    }
   ],
   "source": [
    "# Initialize config and state manager\n",
    "config = NeuroflowConfig.find_and_load()\n",
    "state = StateManager(config)\n",
    "\n",
    "print(\"\\nüóÑÔ∏è Testing Database Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize database (create tables)\n",
    "try:\n",
    "    state.init_db()\n",
    "    print(\"\\n‚úÖ Database initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Database already initialized or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table existence\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(state.engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "print(\"\\nüìä Database Tables:\")\n",
    "expected_tables = ['subjects', 'sessions', 'pipeline_runs', 'workflow_runs', 'audit_logs']\n",
    "for table in expected_tables:\n",
    "    exists = table in tables\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {table}\")\n",
    "\n",
    "# Check workflow_runs columns\n",
    "if 'workflow_runs' in tables:\n",
    "    columns = inspector.get_columns('workflow_runs')\n",
    "    print(\"\\n  workflow_runs columns:\")\n",
    "    for col in columns:\n",
    "        print(f\"    - {col['name']}: {col['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test StateManager Workflow Operations\n",
    "\n",
    "Testing the new workflow-related methods in StateManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Testing StateManager Workflow Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a test workflow run\n",
    "workflow_run = state.create_workflow_run(\n",
    "    trigger_type=\"manual\",\n",
    "    trigger_details={\"user\": \"test\", \"reason\": \"phase1_testing\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created WorkflowRun: ID={workflow_run.id}\")\n",
    "print(f\"  Status: {workflow_run.status.value}\")\n",
    "print(f\"  Trigger: {workflow_run.trigger_type}\")\n",
    "print(f\"  Started: {workflow_run.started_at}\")\n",
    "print(f\"  Stages Completed: {workflow_run.stages_completed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update workflow run\n",
    "print(\"\\nüìù Updating WorkflowRun...\")\n",
    "\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    current_stage=\"discovery\",\n",
    "    sessions_discovered=5,\n",
    ")\n",
    "\n",
    "# Get updated run\n",
    "latest_run = state.get_latest_workflow_run()\n",
    "print(f\"\\n‚úÖ Updated WorkflowRun:\")\n",
    "print(f\"  Current Stage: {latest_run.current_stage}\")\n",
    "print(f\"  Sessions Discovered: {latest_run.sessions_discovered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stage completion\n",
    "print(\"\\nüéØ Simulating Stage Completion...\")\n",
    "\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    current_stage=\"bids_conversion\",\n",
    "    stages_completed=[\"discovery\"],\n",
    "    sessions_converted=3,\n",
    ")\n",
    "\n",
    "# Complete workflow\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    status=WorkflowRunStatus.COMPLETED,\n",
    "    stages_completed=[\"discovery\", \"bids_conversion\"],\n",
    "    current_stage=None,\n",
    ")\n",
    "\n",
    "final_run = state.get_latest_workflow_run()\n",
    "print(f\"\\n‚úÖ Workflow Completed:\")\n",
    "print(f\"  Status: {final_run.status.value}\")\n",
    "print(f\"  Stages Completed: {final_run.stages_completed}\")\n",
    "print(f\"  Duration: {final_run.duration_seconds:.2f} seconds\" if final_run.duration_seconds else \"  Duration: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test workflow history\n",
    "print(\"\\nüìú Workflow Run History:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = state.get_workflow_run_history(limit=5)\n",
    "for i, run in enumerate(history, 1):\n",
    "    print(f\"\\n{i}. WorkflowRun #{run.id}\")\n",
    "    print(f\"   Status: {run.status.value}\")\n",
    "    print(f\"   Trigger: {run.trigger_type}\")\n",
    "    print(f\"   Started: {run.started_at}\")\n",
    "    print(f\"   Stages: {', '.join(run.stages_completed) if run.stages_completed else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Enhanced Model Fields\n",
    "\n",
    "Testing the new workflow tracking fields added to Session and Subject models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Testing Enhanced Model Fields\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test subject and session\n",
    "subject = state.get_or_create_subject(\n",
    "    participant_id=\"sub-TEST001\",\n",
    "    recruitment_id=\"REC-001\"\n",
    ")\n",
    "\n",
    "session = state.register_session(\n",
    "    subject_id=subject.id,\n",
    "    session_id=\"ses-01\",\n",
    "    dicom_path=\"/test/dicom/path\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created Test Data:\")\n",
    "print(f\"  Subject: {subject.participant_id} (ID: {subject.id})\")\n",
    "if session:\n",
    "    print(f\"  Session: {session.session_id} (ID: {session.id})\")\n",
    "else:\n",
    "    print(f\"  Session: Already exists, using existing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mark_session_for_rerun\n",
    "print(\"\\nüîÑ Testing mark_session_for_rerun()...\")\n",
    "\n",
    "if session:\n",
    "    state.mark_session_for_rerun(\n",
    "        session.id,\n",
    "        reason=\"Pipeline failed due to memory issue\"\n",
    "    )\n",
    "    \n",
    "    # Verify the flag was set\n",
    "    with state.get_session() as db:\n",
    "        updated_session = db.get(Session, session.id)\n",
    "        print(f\"\\n‚úÖ Session marked for rerun:\")\n",
    "        print(f\"  needs_rerun: {updated_session.needs_rerun}\")\n",
    "        print(f\"  last_failure_reason: {updated_session.last_failure_reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mark_subject_for_qsiprep\n",
    "print(\"\\nüîÑ Testing mark_subject_for_qsiprep()...\")\n",
    "\n",
    "state.mark_subject_for_qsiprep(\n",
    "    subject.id,\n",
    "    reason=\"New session added\"\n",
    ")\n",
    "\n",
    "# Verify the flag was set\n",
    "with state.get_session() as db:\n",
    "    updated_subject = db.get(Subject, subject.id)\n",
    "    print(f\"\\n‚úÖ Subject marked for QSIPrep:\")\n",
    "    print(f\"  needs_qsiprep: {updated_subject.needs_qsiprep}\")\n",
    "    print(f\"  sessions_at_last_qsiprep: {updated_subject.sessions_at_last_qsiprep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PipelineRun with workflow_run_id\n",
    "print(\"\\nüîó Testing PipelineRun with workflow_run_id...\")\n",
    "\n",
    "if session:\n",
    "    # Create a new workflow run\n",
    "    test_workflow = state.create_workflow_run(\n",
    "        trigger_type=\"test\",\n",
    "        trigger_details={\"testing\": \"pipeline_association\"}\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline run\n",
    "    pipeline_run = state.create_pipeline_run(\n",
    "        pipeline_name=\"bids_conversion\",\n",
    "        pipeline_level=\"session\",\n",
    "        session_id=session.id,\n",
    "        subject_id=subject.id,\n",
    "        trigger_reason=\"workflow\",\n",
    "    )\n",
    "    \n",
    "    # Link it to the workflow\n",
    "    with state.get_session() as db:\n",
    "        run = db.get(PipelineRun, pipeline_run.id)\n",
    "        run.workflow_run_id = test_workflow.id\n",
    "        db.commit()\n",
    "    \n",
    "    # Verify the link\n",
    "    with state.get_session() as db:\n",
    "        linked_run = db.get(PipelineRun, pipeline_run.id)\n",
    "        print(f\"\\n‚úÖ PipelineRun linked to WorkflowRun:\")\n",
    "        print(f\"  PipelineRun ID: {linked_run.id}\")\n",
    "        print(f\"  WorkflowRun ID: {linked_run.workflow_run_id}\")\n",
    "        print(f\"  Pipeline: {linked_run.pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Celery Configuration\n",
    "\n",
    "Testing the enhanced Celery configuration with queues and routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüêù Testing Celery Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from neuroflow.workers.celery_app import create_celery_app\n",
    "    \n",
    "    celery_app = create_celery_app(config)\n",
    "    \n",
    "    print(\"\\n‚úÖ Celery app created successfully!\")\n",
    "    print(f\"  Broker: {celery_app.conf.broker_url}\")\n",
    "    print(f\"  Backend: {celery_app.conf.result_backend}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not create Celery app: {e}\")\n",
    "    print(\"   (This is expected if Redis is not running)\")\n",
    "    celery_app = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display task queues\n",
    "if celery_app:\n",
    "    print(\"\\nüìã Task Queues:\")\n",
    "    for queue in celery_app.conf.task_queues:\n",
    "        priority = queue.queue_arguments.get('x-max-priority', 'N/A')\n",
    "        print(f\"  - {queue.name}\")\n",
    "        print(f\"    Routing Key: {queue.routing_key}\")\n",
    "        print(f\"    Priority: {priority}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display task routes\n",
    "if celery_app:\n",
    "    print(\"\\nüó∫Ô∏è Task Routes:\")\n",
    "    routes = celery_app.conf.task_routes\n",
    "    for task_name, route_config in list(routes.items())[:10]:  # Show first 10\n",
    "        print(f\"  {task_name}\")\n",
    "        print(f\"    ‚Üí Queue: {route_config['queue']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display beat schedule\n",
    "if celery_app:\n",
    "    print(\"\\n‚è∞ Celery Beat Schedule:\")\n",
    "    for name, schedule_config in celery_app.conf.beat_schedule.items():\n",
    "        print(f\"\\n  {name}:\")\n",
    "        print(f\"    Task: {schedule_config['task']}\")\n",
    "        print(f\"    Schedule: {schedule_config['schedule']}\")\n",
    "        if 'options' in schedule_config:\n",
    "            print(f\"    Queue: {schedule_config['options'].get('queue', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Workflow Tasks (Import Check)\n",
    "\n",
    "Verify that workflow tasks are properly registered and importable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Testing Workflow Tasks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from neuroflow.workers import workflow_tasks\n",
    "    \n",
    "    print(\"\\n‚úÖ Workflow tasks module imported successfully!\")\n",
    "    \n",
    "    # List available tasks\n",
    "    task_functions = [\n",
    "        name for name in dir(workflow_tasks) \n",
    "        if name.startswith('run_') and callable(getattr(workflow_tasks, name))\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìã Available Workflow Tasks:\")\n",
    "    for task_name in task_functions:\n",
    "        task_func = getattr(workflow_tasks, task_name)\n",
    "        # Get task name from shared_task decorator\n",
    "        if hasattr(task_func, 'name'):\n",
    "            print(f\"  - {task_func.name}\")\n",
    "        else:\n",
    "            print(f\"  - {task_name}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error importing workflow tasks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Audit Log\n",
    "\n",
    "Review all operations that were logged during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Phase 1 Testing Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count workflow runs\n",
    "with state.get_session() as db:\n",
    "    from sqlalchemy import func\n",
    "    \n",
    "    workflow_count = db.execute(\n",
    "        select(func.count(WorkflowRun.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    subject_count = db.execute(\n",
    "        select(func.count(Subject.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    session_count = db.execute(\n",
    "        select(func.count(Session.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Database State:\")\n",
    "    print(f\"  Workflow Runs: {workflow_count}\")\n",
    "    print(f\"  Subjects: {subject_count}\")\n",
    "    print(f\"  Sessions: {session_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show recent audit log entries\n",
    "print(\"\\nüìú Recent Audit Log Entries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with state.get_session() as db:\n",
    "    recent_logs = db.execute(\n",
    "        select(AuditLog)\n",
    "        .order_by(AuditLog.created_at.desc())\n",
    "        .limit(10)\n",
    "    ).scalars().all()\n",
    "    \n",
    "    for log in recent_logs:\n",
    "        print(f\"\\n  [{log.created_at.strftime('%H:%M:%S')}] {log.entity_type}.{log.action}\")\n",
    "        print(f\"    Entity ID: {log.entity_id}\")\n",
    "        if log.message:\n",
    "            print(f\"    Message: {log.message}\")\n",
    "        if log.old_value and log.new_value:\n",
    "            print(f\"    Change: {log.old_value} ‚Üí {log.new_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Phase 1 Testing Complete!\n",
    "\n",
    "### Components Verified:\n",
    "‚úÖ Pipeline Registry with dependency management\n",
    "‚úÖ WorkflowRun model and database schema\n",
    "‚úÖ Enhanced Session/Subject models with workflow tracking\n",
    "‚úÖ StateManager workflow operations\n",
    "‚úÖ Celery configuration with queues and routing\n",
    "‚úÖ Workflow tasks module\n",
    "‚úÖ Audit logging for all operations\n",
    "\n",
    "### Next Steps:\n",
    "- Phase 2: Implement WorkflowScheduler\n",
    "- Phase 3: Error handling and retry logic\n",
    "- Phase 4: Logging and monitoring\n",
    "- Phase 5: CLI commands and operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
