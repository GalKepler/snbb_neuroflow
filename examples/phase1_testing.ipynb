{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Core Infrastructure Testing\n",
    "\n",
    "This notebook tests all components added in Phase 1 of the workflow orchestration system.\n",
    "\n",
    "## Components Tested:\n",
    "1. Pipeline Registry\n",
    "2. WorkflowRun Model\n",
    "3. Enhanced Models (Session, Subject, PipelineRun)\n",
    "4. StateManager Workflow Operations\n",
    "5. Celery Configuration\n",
    "\n",
    "**Prerequisites:**\n",
    "- Database initialized\n",
    "- neuroflow.yaml configured\n",
    "- Redis running (for Celery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from neuroflow.config import NeuroflowConfig\n",
    "from neuroflow.core.state import StateManager\n",
    "from neuroflow.models import (\n",
    "    WorkflowRun,\n",
    "    WorkflowRunStatus,\n",
    "    Session,\n",
    "    SessionStatus,\n",
    "    Subject,\n",
    "    SubjectStatus,\n",
    "    PipelineRun,\n",
    "    PipelineRunStatus,\n",
    ")\n",
    "from neuroflow.orchestrator.registry import (\n",
    "    get_registry,\n",
    "    PipelineRegistry,\n",
    "    PipelineLevel,\n",
    "    PipelineStage,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Pipeline Registry\n",
    "\n",
    "The Pipeline Registry provides centralized pipeline definitions with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Pipeline Registry\n",
      "============================================================\n",
      "\n",
      "üìã Registered Pipelines:\n",
      "  - bids_conversion: BIDS Conversion (HeudiConv)\n",
      "    Stage: BIDS_CONVERSION, Level: session\n",
      "    Queue: bids, Timeout: 60min\n",
      "\n",
      "  - qsiprep: QSIPrep Preprocessing\n",
      "    Stage: PREPROCESSING, Level: subject\n",
      "    Queue: heavy_processing, Timeout: 1440min\n",
      "    Depends on: bids_conversion\n",
      "\n",
      "  - qsirecon: QSIRecon Reconstruction\n",
      "    Stage: RECONSTRUCTION, Level: session\n",
      "    Queue: processing, Timeout: 720min\n",
      "    Depends on: qsiprep\n",
      "\n",
      "  - qsiparc: QSIParc Parcellation\n",
      "    Stage: PARCELLATION, Level: session\n",
      "    Queue: processing, Timeout: 240min\n",
      "    Depends on: qsirecon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the global registry instance\n",
    "registry = get_registry()\n",
    "\n",
    "print(\"\\nüîç Testing Pipeline Registry\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all registered pipelines\n",
    "print(\"\\nüìã Registered Pipelines:\")\n",
    "for pipeline in registry.get_all():\n",
    "    print(f\"  - {pipeline.name}: {pipeline.display_name}\")\n",
    "    print(f\"    Stage: {pipeline.stage.name}, Level: {pipeline.level.value}\")\n",
    "    print(f\"    Queue: {pipeline.queue}, Timeout: {pipeline.timeout_minutes}min\")\n",
    "    if pipeline.depends_on:\n",
    "        print(f\"    Depends on: {', '.join(pipeline.depends_on)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Testing Dependency Resolution\n",
      "============================================================\n",
      "\n",
      "Dependencies for 'qsiparc':\n",
      "  - bids_conversion (Stage 1)\n",
      "  - qsiprep (Stage 2)\n",
      "  - qsirecon (Stage 3)\n",
      "\n",
      "üìä Execution Order (Topological Sort):\n",
      "  1. bids_conversion (Stage: BIDS_CONVERSION)\n",
      "  2. qsiprep (Stage: PREPROCESSING)\n",
      "  3. qsirecon (Stage: RECONSTRUCTION)\n",
      "  4. qsiparc (Stage: PARCELLATION)\n"
     ]
    }
   ],
   "source": [
    "# Test dependency resolution\n",
    "print(\"\\nüîó Testing Dependency Resolution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline_name = \"qsiparc\"\n",
    "dependencies = registry.get_dependencies(pipeline_name)\n",
    "print(f\"\\nDependencies for '{pipeline_name}':\")\n",
    "for dep in dependencies:\n",
    "    print(f\"  - {dep.name} (Stage {dep.stage.value})\")\n",
    "\n",
    "# Test execution order (topological sort)\n",
    "print(\"\\nüìä Execution Order (Topological Sort):\")\n",
    "execution_order = registry.get_execution_order()\n",
    "for i, pipeline in enumerate(execution_order, 1):\n",
    "    print(f\"  {i}. {pipeline.name} (Stage: {pipeline.stage.name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Validating Dependencies\n",
      "============================================================\n",
      "\n",
      "‚úÖ All dependencies valid!\n"
     ]
    }
   ],
   "source": [
    "# Validate dependencies\n",
    "print(\"\\n‚úì Validating Dependencies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = registry.validate_dependencies()\n",
    "if errors:\n",
    "    print(\"\\n‚ùå Dependency Errors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All dependencies valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è Testing Retry Policies\n",
      "============================================================\n",
      "\n",
      "QSIPrep Retry Policy:\n",
      "  Max attempts: 3\n",
      "  Initial delay: 1800s\n",
      "  Max delay: 7200s\n",
      "  Exponential backoff: True\n",
      "\n",
      "  Delay schedule:\n",
      "    Attempt 1: 1800s (30.0 minutes)\n",
      "    Attempt 2: 3600s (60.0 minutes)\n",
      "    Attempt 3: 7200s (120.0 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Test retry policy\n",
    "print(\"\\n‚è±Ô∏è Testing Retry Policies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qsiprep = registry.get(\"qsiprep\")\n",
    "print(f\"\\nQSIPrep Retry Policy:\")\n",
    "print(f\"  Max attempts: {qsiprep.retry_policy.max_attempts}\")\n",
    "print(f\"  Initial delay: {qsiprep.retry_policy.initial_delay_seconds}s\")\n",
    "print(f\"  Max delay: {qsiprep.retry_policy.max_delay_seconds}s\")\n",
    "print(f\"  Exponential backoff: {qsiprep.retry_policy.exponential_backoff}\")\n",
    "\n",
    "print(\"\\n  Delay schedule:\")\n",
    "for attempt in range(1, qsiprep.retry_policy.max_attempts + 1):\n",
    "    delay = qsiprep.retry_policy.get_delay(attempt)\n",
    "    print(f\"    Attempt {attempt}: {delay}s ({delay/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíª Testing Resource Requirements\n",
      "============================================================\n",
      "\n",
      "BIDS Conversion (HeudiConv):\n",
      "  CPUs: 1-2\n",
      "  Memory: 4-8 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 30 minutes\n",
      "\n",
      "QSIPrep Preprocessing:\n",
      "  CPUs: 8-16\n",
      "  Memory: 16-32 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 720 minutes\n",
      "\n",
      "QSIRecon Reconstruction:\n",
      "  CPUs: 4-8\n",
      "  Memory: 8-16 GB\n",
      "  GPU Required: False\n",
      "  Est. Duration: 480 minutes\n"
     ]
    }
   ],
   "source": [
    "# Test resource requirements\n",
    "print(\"\\nüíª Testing Resource Requirements\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pipeline_name in [\"bids_conversion\", \"qsiprep\", \"qsirecon\"]:\n",
    "    pipeline = registry.get(pipeline_name)\n",
    "    resources = pipeline.resources\n",
    "    print(f\"\\n{pipeline.display_name}:\")\n",
    "    print(f\"  CPUs: {resources.min_cpus}-{resources.max_cpus}\")\n",
    "    print(f\"  Memory: {resources.min_memory_gb}-{resources.max_memory_gb} GB\")\n",
    "    print(f\"  GPU Required: {resources.requires_gpu}\")\n",
    "    print(f\"  Est. Duration: {resources.estimated_duration_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Database Models and Migrations\n",
    "\n",
    "Testing the new WorkflowRun model and enhanced Session/Subject models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üóÑÔ∏è Testing Database Models\n",
      "============================================================\n",
      "\u001b[2m2026-02-07 19:33:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdatabase.initialized          \u001b[0m \u001b[36murl\u001b[0m=\u001b[35msqlite:////media/storage/yalab-dev/snbb_neuroflow/.neuroflow/neuroflow.db\u001b[0m\n",
      "\n",
      "‚úÖ Database initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize config and state manager\n",
    "config = NeuroflowConfig.find_and_load()\n",
    "state = StateManager(config)\n",
    "\n",
    "print(\"\\nüóÑÔ∏è Testing Database Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize database (create tables)\n",
    "try:\n",
    "    state.init_db()\n",
    "    print(\"\\n‚úÖ Database initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Database already initialized or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Database Tables:\n",
      "  ‚úÖ subjects\n",
      "  ‚úÖ sessions\n",
      "  ‚úÖ pipeline_runs\n",
      "  ‚úÖ workflow_runs\n",
      "  ‚ùå audit_logs\n",
      "\n",
      "  workflow_runs columns:\n",
      "    - id: INTEGER\n",
      "    - status: VARCHAR(32)\n",
      "    - trigger_type: VARCHAR(32)\n",
      "    - trigger_details: JSON\n",
      "    - started_at: DATETIME\n",
      "    - completed_at: DATETIME\n",
      "    - stages_completed: JSON\n",
      "    - current_stage: VARCHAR(64)\n",
      "    - sessions_discovered: INTEGER\n",
      "    - sessions_converted: INTEGER\n",
      "    - subjects_preprocessed: INTEGER\n",
      "    - sessions_reconstructed: INTEGER\n",
      "    - sessions_parcellated: INTEGER\n",
      "    - error_message: TEXT\n",
      "    - error_stage: VARCHAR(64)\n",
      "    - error_details: JSON\n",
      "    - created_at: DATETIME\n",
      "    - updated_at: DATETIME\n"
     ]
    }
   ],
   "source": [
    "# Verify table existence\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(state.engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "print(\"\\nüìä Database Tables:\")\n",
    "expected_tables = ['subjects', 'sessions', 'pipeline_runs', 'workflow_runs', 'audit_logs']\n",
    "for table in expected_tables:\n",
    "    exists = table in tables\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {table}\")\n",
    "\n",
    "# Check workflow_runs columns\n",
    "if 'workflow_runs' in tables:\n",
    "    columns = inspector.get_columns('workflow_runs')\n",
    "    print(\"\\n  workflow_runs columns:\")\n",
    "    for col in columns:\n",
    "        print(f\"    - {col['name']}: {col['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test StateManager Workflow Operations\n",
    "\n",
    "Testing the new workflow-related methods in StateManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Testing StateManager Workflow Operations\n",
      "============================================================\n",
      "\u001b[2m2026-02-07 19:33:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mworkflow_run.created          \u001b[0m \u001b[36mtrigger_type\u001b[0m=\u001b[35mmanual\u001b[0m \u001b[36mworkflow_run_id\u001b[0m=\u001b[35m1\u001b[0m\n",
      "\n",
      "‚úÖ Created WorkflowRun: ID=1\n",
      "  Status: running\n",
      "  Trigger: manual\n",
      "  Started: 2026-02-07 17:33:52.787238+00:00\n",
      "  Stages Completed: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÑ Testing StateManager Workflow Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a test workflow run\n",
    "workflow_run = state.create_workflow_run(\n",
    "    trigger_type=\"manual\",\n",
    "    trigger_details={\"user\": \"test\", \"reason\": \"phase1_testing\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created WorkflowRun: ID={workflow_run.id}\")\n",
    "print(f\"  Status: {workflow_run.status.value}\")\n",
    "print(f\"  Trigger: {workflow_run.trigger_type}\")\n",
    "print(f\"  Started: {workflow_run.started_at}\")\n",
    "print(f\"  Stages Completed: {workflow_run.stages_completed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Updating WorkflowRun...\n",
      "\n",
      "‚úÖ Updated WorkflowRun:\n",
      "  Current Stage: discovery\n",
      "  Sessions Discovered: 5\n"
     ]
    }
   ],
   "source": [
    "# Update workflow run\n",
    "print(\"\\nüìù Updating WorkflowRun...\")\n",
    "\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    current_stage=\"discovery\",\n",
    "    sessions_discovered=5,\n",
    ")\n",
    "\n",
    "# Get updated run\n",
    "latest_run = state.get_latest_workflow_run()\n",
    "print(f\"\\n‚úÖ Updated WorkflowRun:\")\n",
    "print(f\"  Current Stage: {latest_run.current_stage}\")\n",
    "print(f\"  Sessions Discovered: {latest_run.sessions_discovered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Simulating Stage Completion...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m state.update_workflow_run(\n\u001b[32m      5\u001b[39m     workflow_run.id,\n\u001b[32m      6\u001b[39m     current_stage=\u001b[33m\"\u001b[39m\u001b[33mbids_conversion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     stages_completed=[\u001b[33m\"\u001b[39m\u001b[33mdiscovery\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     sessions_converted=\u001b[32m3\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Complete workflow\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_workflow_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkflow_run\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWorkflowRunStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstages_completed\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdiscovery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbids_conversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m final_run = state.get_latest_workflow_run()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Workflow Completed:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/neuroflow/neuroflow/core/state.py:569\u001b[39m, in \u001b[36mStateManager.update_workflow_run\u001b[39m\u001b[34m(self, workflow_run_id, status, current_stage, stages_completed, error_message, error_stage, **metrics)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run:\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m old_status = \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status:\n\u001b[32m    572\u001b[39m     run.status = status\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "# Simulate stage completion\n",
    "print(\"\\nüéØ Simulating Stage Completion...\")\n",
    "\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    current_stage=\"bids_conversion\",\n",
    "    stages_completed=[\"discovery\"],\n",
    "    sessions_converted=3,\n",
    ")\n",
    "\n",
    "# Complete workflow\n",
    "state.update_workflow_run(\n",
    "    workflow_run.id,\n",
    "    status=WorkflowRunStatus.COMPLETED,\n",
    "    stages_completed=[\"discovery\", \"bids_conversion\"],\n",
    "    current_stage=None,\n",
    ")\n",
    "\n",
    "final_run = state.get_latest_workflow_run()\n",
    "print(f\"\\n‚úÖ Workflow Completed:\")\n",
    "print(f\"  Status: {final_run.status.value}\")\n",
    "print(f\"  Stages Completed: {final_run.stages_completed}\")\n",
    "print(f\"  Duration: {final_run.duration_seconds:.2f} seconds\" if final_run.duration_seconds else \"  Duration: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú Workflow Run History:\n",
      "============================================================\n",
      "\n",
      "1. WorkflowRun #1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(history, \u001b[32m1\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. WorkflowRun #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Trigger: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.trigger_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Started: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.started_at\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "# Test workflow history\n",
    "print(\"\\nüìú Workflow Run History:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = state.get_workflow_run_history(limit=5)\n",
    "for i, run in enumerate(history, 1):\n",
    "    print(f\"\\n{i}. WorkflowRun #{run.id}\")\n",
    "    print(f\"   Status: {run.status.value}\")\n",
    "    print(f\"   Trigger: {run.trigger_type}\")\n",
    "    print(f\"   Started: {run.started_at}\")\n",
    "    print(f\"   Stages: {', '.join(run.stages_completed) if run.stages_completed else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Enhanced Model Fields\n",
    "\n",
    "Testing the new workflow tracking fields added to Session and Subject models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Enhanced Model Fields\n",
      "============================================================\n",
      "\u001b[2m2026-02-07 19:34:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1msubject.created               \u001b[0m \u001b[36mparticipant_id\u001b[0m=\u001b[35msub-TEST001\u001b[0m\n",
      "\u001b[2m2026-02-07 19:34:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1msession.registered            \u001b[0m \u001b[36mdicom_path\u001b[0m=\u001b[35m/test/dicom/path\u001b[0m \u001b[36msession_id\u001b[0m=\u001b[35mses-01\u001b[0m \u001b[36msubject_id\u001b[0m=\u001b[35m1\u001b[0m\n",
      "\n",
      "‚úÖ Created Test Data:\n",
      "  Subject: sub-TEST001 (ID: 1)\n",
      "  Session: ses-01 (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Testing Enhanced Model Fields\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test subject and session\n",
    "subject = state.get_or_create_subject(\n",
    "    participant_id=\"sub-TEST001\",\n",
    "    recruitment_id=\"REC-001\"\n",
    ")\n",
    "\n",
    "session = state.register_session(\n",
    "    subject_id=subject.id,\n",
    "    session_id=\"ses-01\",\n",
    "    dicom_path=\"/test/dicom/path\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created Test Data:\")\n",
    "print(f\"  Subject: {subject.participant_id} (ID: {subject.id})\")\n",
    "if session:\n",
    "    print(f\"  Session: {session.session_id} (ID: {session.id})\")\n",
    "else:\n",
    "    print(f\"  Session: Already exists, using existing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Testing mark_session_for_rerun()...\n",
      "\n",
      "‚úÖ Session marked for rerun:\n",
      "  needs_rerun: True\n",
      "  last_failure_reason: Pipeline failed due to memory issue\n"
     ]
    }
   ],
   "source": [
    "# Test mark_session_for_rerun\n",
    "print(\"\\nüîÑ Testing mark_session_for_rerun()...\")\n",
    "\n",
    "if session:\n",
    "    state.mark_session_for_rerun(\n",
    "        session.id,\n",
    "        reason=\"Pipeline failed due to memory issue\"\n",
    "    )\n",
    "    \n",
    "    # Verify the flag was set\n",
    "    with state.get_session() as db:\n",
    "        updated_session = db.get(Session, session.id)\n",
    "        print(f\"\\n‚úÖ Session marked for rerun:\")\n",
    "        print(f\"  needs_rerun: {updated_session.needs_rerun}\")\n",
    "        print(f\"  last_failure_reason: {updated_session.last_failure_reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Testing mark_subject_for_qsiprep()...\n",
      "\n",
      "‚úÖ Subject marked for QSIPrep:\n",
      "  needs_qsiprep: True\n",
      "  sessions_at_last_qsiprep: 0\n"
     ]
    }
   ],
   "source": [
    "# Test mark_subject_for_qsiprep\n",
    "print(\"\\nüîÑ Testing mark_subject_for_qsiprep()...\")\n",
    "\n",
    "state.mark_subject_for_qsiprep(\n",
    "    subject.id,\n",
    "    reason=\"New session added\"\n",
    ")\n",
    "\n",
    "# Verify the flag was set\n",
    "with state.get_session() as db:\n",
    "    updated_subject = db.get(Subject, subject.id)\n",
    "    print(f\"\\n‚úÖ Subject marked for QSIPrep:\")\n",
    "    print(f\"  needs_qsiprep: {updated_subject.needs_qsiprep}\")\n",
    "    print(f\"  sessions_at_last_qsiprep: {updated_subject.sessions_at_last_qsiprep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Testing PipelineRun with workflow_run_id...\n",
      "\u001b[2m2026-02-07 19:35:01\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mworkflow_run.created          \u001b[0m \u001b[36mtrigger_type\u001b[0m=\u001b[35mtest\u001b[0m \u001b[36mworkflow_run_id\u001b[0m=\u001b[35m2\u001b[0m\n",
      "\u001b[2m2026-02-07 19:35:01\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mpipeline_run.created          \u001b[0m \u001b[36mattempt\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mpipeline\u001b[0m=\u001b[35mbids_conversion\u001b[0m \u001b[36mrun_id\u001b[0m=\u001b[35m1\u001b[0m\n",
      "\n",
      "‚úÖ PipelineRun linked to WorkflowRun:\n",
      "  PipelineRun ID: 1\n",
      "  WorkflowRun ID: 2\n",
      "  Pipeline: bids_conversion\n"
     ]
    }
   ],
   "source": [
    "# Test PipelineRun with workflow_run_id\n",
    "print(\"\\nüîó Testing PipelineRun with workflow_run_id...\")\n",
    "\n",
    "if session:\n",
    "    # Create a new workflow run\n",
    "    test_workflow = state.create_workflow_run(\n",
    "        trigger_type=\"test\",\n",
    "        trigger_details={\"testing\": \"pipeline_association\"}\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline run\n",
    "    pipeline_run = state.create_pipeline_run(\n",
    "        pipeline_name=\"bids_conversion\",\n",
    "        pipeline_level=\"session\",\n",
    "        session_id=session.id,\n",
    "        subject_id=subject.id,\n",
    "        trigger_reason=\"workflow\",\n",
    "    )\n",
    "    \n",
    "    # Link it to the workflow\n",
    "    with state.get_session() as db:\n",
    "        run = db.get(PipelineRun, pipeline_run.id)\n",
    "        run.workflow_run_id = test_workflow.id\n",
    "        db.commit()\n",
    "    \n",
    "    # Verify the link\n",
    "    with state.get_session() as db:\n",
    "        linked_run = db.get(PipelineRun, pipeline_run.id)\n",
    "        print(f\"\\n‚úÖ PipelineRun linked to WorkflowRun:\")\n",
    "        print(f\"  PipelineRun ID: {linked_run.id}\")\n",
    "        print(f\"  WorkflowRun ID: {linked_run.workflow_run_id}\")\n",
    "        print(f\"  Pipeline: {linked_run.pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Celery Configuration\n",
    "\n",
    "Testing the enhanced Celery configuration with queues and routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêù Testing Celery Configuration\n",
      "============================================================\n",
      "\n",
      "‚úÖ Celery app created successfully!\n",
      "  Broker: redis://localhost:6379/0\n",
      "  Backend: redis://localhost:6379/0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüêù Testing Celery Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from neuroflow.workers.celery_app import create_celery_app\n",
    "    \n",
    "    celery_app = create_celery_app(config)\n",
    "    \n",
    "    print(\"\\n‚úÖ Celery app created successfully!\")\n",
    "    print(f\"  Broker: {celery_app.conf.broker_url}\")\n",
    "    print(f\"  Backend: {celery_app.conf.result_backend}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Could not create Celery app: {e}\")\n",
    "    print(\"   (This is expected if Redis is not running)\")\n",
    "    celery_app = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Task Queues:\n",
      "  - workflow\n",
      "    Routing Key: workflow\n",
      "    Priority: 10\n",
      "\n",
      "  - discovery\n",
      "    Routing Key: discovery\n",
      "    Priority: 8\n",
      "\n",
      "  - bids\n",
      "    Routing Key: bids\n",
      "    Priority: 5\n",
      "\n",
      "  - heavy_processing\n",
      "    Routing Key: heavy\n",
      "    Priority: 2\n",
      "\n",
      "  - processing\n",
      "    Routing Key: processing\n",
      "    Priority: 4\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã Task Queues:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m queue \u001b[38;5;129;01min\u001b[39;00m celery_app.conf.task_queues:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     priority = \u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqueue_arguments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mx-max-priority\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqueue.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Routing Key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqueue.routing_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Display task queues\n",
    "if celery_app:\n",
    "    print(\"\\nüìã Task Queues:\")\n",
    "    for queue in celery_app.conf.task_queues:\n",
    "        priority = queue.queue_arguments.get('x-max-priority', 'N/A')\n",
    "        print(f\"  - {queue.name}\")\n",
    "        print(f\"    Routing Key: {queue.routing_key}\")\n",
    "        print(f\"    Priority: {priority}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó∫Ô∏è Task Routes:\n",
      "  neuroflow.workers.workflow_tasks.run_master_workflow\n",
      "    ‚Üí Queue: workflow\n",
      "  neuroflow.workers.workflow_tasks.run_discovery_stage\n",
      "    ‚Üí Queue: discovery\n",
      "  neuroflow.workers.workflow_tasks.run_bids_stage\n",
      "    ‚Üí Queue: bids\n",
      "  neuroflow.workers.workflow_tasks.run_qsiprep_stage\n",
      "    ‚Üí Queue: heavy_processing\n",
      "  neuroflow.workers.workflow_tasks.run_qsirecon_stage\n",
      "    ‚Üí Queue: processing\n",
      "  neuroflow.workers.workflow_tasks.run_qsiparc_stage\n",
      "    ‚Üí Queue: processing\n",
      "  neuroflow.workers.tasks.scan_directories\n",
      "    ‚Üí Queue: discovery\n",
      "  neuroflow.workers.tasks.validate_session\n",
      "    ‚Üí Queue: discovery\n",
      "  neuroflow.workers.tasks.run_bids_conversion\n",
      "    ‚Üí Queue: bids\n"
     ]
    }
   ],
   "source": [
    "# Display task routes\n",
    "if celery_app:\n",
    "    print(\"\\nüó∫Ô∏è Task Routes:\")\n",
    "    routes = celery_app.conf.task_routes\n",
    "    for task_name, route_config in list(routes.items())[:10]:  # Show first 10\n",
    "        print(f\"  {task_name}\")\n",
    "        print(f\"    ‚Üí Queue: {route_config['queue']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ Celery Beat Schedule:\n",
      "\n",
      "  master-workflow:\n",
      "    Task: neuroflow.workers.workflow_tasks.run_master_workflow\n",
      "    Schedule: <crontab: 0 2 */3 * * (m/h/dM/MY/d)>\n",
      "    Queue: workflow\n",
      "\n",
      "  hourly-scan:\n",
      "    Task: neuroflow.workers.tasks.scan_directories\n",
      "    Schedule: <crontab: 0 * * * * (m/h/dM/MY/d)>\n",
      "    Queue: discovery\n",
      "\n",
      "  daily-cleanup:\n",
      "    Task: neuroflow.workers.tasks.cleanup_stale_runs\n",
      "    Schedule: <crontab: 0 3 * * * (m/h/dM/MY/d)>\n",
      "    Queue: default\n"
     ]
    }
   ],
   "source": [
    "# Display beat schedule\n",
    "if celery_app:\n",
    "    print(\"\\n‚è∞ Celery Beat Schedule:\")\n",
    "    for name, schedule_config in celery_app.conf.beat_schedule.items():\n",
    "        print(f\"\\n  {name}:\")\n",
    "        print(f\"    Task: {schedule_config['task']}\")\n",
    "        print(f\"    Schedule: {schedule_config['schedule']}\")\n",
    "        if 'options' in schedule_config:\n",
    "            print(f\"    Queue: {schedule_config['options'].get('queue', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Workflow Tasks (Import Check)\n",
    "\n",
    "Verify that workflow tasks are properly registered and importable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Testing Workflow Tasks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from neuroflow.workers import workflow_tasks\n",
    "    \n",
    "    print(\"\\n‚úÖ Workflow tasks module imported successfully!\")\n",
    "    \n",
    "    # List available tasks\n",
    "    task_functions = [\n",
    "        name for name in dir(workflow_tasks) \n",
    "        if name.startswith('run_') and callable(getattr(workflow_tasks, name))\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìã Available Workflow Tasks:\")\n",
    "    for task_name in task_functions:\n",
    "        task_func = getattr(workflow_tasks, task_name)\n",
    "        # Get task name from shared_task decorator\n",
    "        if hasattr(task_func, 'name'):\n",
    "            print(f\"  - {task_func.name}\")\n",
    "        else:\n",
    "            print(f\"  - {task_name}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error importing workflow tasks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Audit Log\n",
    "\n",
    "Review all operations that were logged during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Phase 1 Testing Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count workflow runs\n",
    "with state.get_session() as db:\n",
    "    from sqlalchemy import func\n",
    "    \n",
    "    workflow_count = db.execute(\n",
    "        select(func.count(WorkflowRun.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    subject_count = db.execute(\n",
    "        select(func.count(Subject.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    session_count = db.execute(\n",
    "        select(func.count(Session.id))\n",
    "    ).scalar()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Database State:\")\n",
    "    print(f\"  Workflow Runs: {workflow_count}\")\n",
    "    print(f\"  Subjects: {subject_count}\")\n",
    "    print(f\"  Sessions: {session_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show recent audit log entries\n",
    "print(\"\\nüìú Recent Audit Log Entries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with state.get_session() as db:\n",
    "    recent_logs = db.execute(\n",
    "        select(AuditLog)\n",
    "        .order_by(AuditLog.created_at.desc())\n",
    "        .limit(10)\n",
    "    ).scalars().all()\n",
    "    \n",
    "    for log in recent_logs:\n",
    "        print(f\"\\n  [{log.created_at.strftime('%H:%M:%S')}] {log.entity_type}.{log.action}\")\n",
    "        print(f\"    Entity ID: {log.entity_id}\")\n",
    "        if log.message:\n",
    "            print(f\"    Message: {log.message}\")\n",
    "        if log.old_value and log.new_value:\n",
    "            print(f\"    Change: {log.old_value} ‚Üí {log.new_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Phase 1 Testing Complete!\n",
    "\n",
    "### Components Verified:\n",
    "‚úÖ Pipeline Registry with dependency management\n",
    "‚úÖ WorkflowRun model and database schema\n",
    "‚úÖ Enhanced Session/Subject models with workflow tracking\n",
    "‚úÖ StateManager workflow operations\n",
    "‚úÖ Celery configuration with queues and routing\n",
    "‚úÖ Workflow tasks module\n",
    "‚úÖ Audit logging for all operations\n",
    "\n",
    "### Next Steps:\n",
    "- Phase 2: Implement WorkflowScheduler\n",
    "- Phase 3: Error handling and retry logic\n",
    "- Phase 4: Logging and monitoring\n",
    "- Phase 5: CLI commands and operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
